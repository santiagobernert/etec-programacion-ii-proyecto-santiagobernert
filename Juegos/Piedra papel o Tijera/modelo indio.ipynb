{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modelo indio.ipynb","provenance":[],"authorship_tag":"ABX9TyPrh4aRP1WuBT/ltFcun43g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"3DHQDXUSzQsD"},"source":["!pip install keras-squeezenet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"id":"FxCfI-yKxp_X","executionInfo":{"status":"error","timestamp":1635260408273,"user_tz":180,"elapsed":309,"user":{"displayName":"Santiago Bernert Diaz","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07929867446317438562"}},"outputId":"6e5459b7-d0db-4a70-bc43-ed18f41a605e"},"source":["import cv2\n","import numpy as np\n","\n","import keras\n","from keras.preprocessing import image\n","from keras.squeezenet import SqueezeNet\n","from keras.optimizers import Adam\n","from keras.utils import np_utils\n","from keras.layers import Activation, Dropout, Convolution2D, GlobalAveragePooling2D\n","from keras.models import Sequential\n","import tensorflow as tf\n","import os\n","\n","IMG_SAVE_PATH = 'image_data'\n","\n","CLASS_MAP = {\n","    \"rock\": 0,\n","    \"paper\": 1,\n","    \"scissors\": 2,\n","    \"none\": 3\n","}\n","\n","NUM_CLASSES = len(CLASS_MAP)\n","\n","\n","def mapper(val):\n","    return CLASS_MAP[val]\n","\n","\n","def get_model():\n","    model = Sequential([\n","        SqueezeNet(input_shape=(300, 300, 3), include_top=False),\n","        Dropout(0.5),\n","        Convolution2D(NUM_CLASSES, (1, 1), padding='valid'),\n","        Activation('relu'),\n","        GlobalAveragePooling2D(),\n","        Activation('softmax')\n","    ])\n","    return model\n","\n","\n","# load images from the directory\n","dataset = []\n","for directory in os.listdir(IMG_SAVE_PATH):\n","    path = os.path.join(IMG_SAVE_PATH, directory)\n","    if not os.path.isdir(path):\n","        continue\n","    for item in os.listdir(path):\n","        # to make sure no hidden files get in our way\n","        if item.startswith(\".\"):\n","            continue\n","        img = cv2.imread(os.path.join(path, item))\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        #img = cv2.resize(img, (227, 227))\n","        dataset.append([img, directory])\n","\n","'''\n","dataset = [\n","    [[...], 'rock'],\n","    [[...], 'paper'],\n","    ...\n","]\n","'''\n","data, labels = zip(*dataset)\n","labels = list(map(mapper, labels))\n","\n","\n","'''\n","labels: rock,paper,paper,scissors,rock...\n","one hot encoded: [1,0,0], [0,1,0], [0,1,0], [0,0,1], [1,0,0]...\n","'''\n","\n","# one hot encode the labels\n","labels = np_utils.to_categorical(labels)\n","\n","# define the model\n","model = get_model()\n","model.compile(\n","    optimizer=Adam(lr=0.0001),\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# start training\n","model.fit(np.array(data), np.array(labels), epochs=10)\n","\n","# save the model for later use\n","model.save(\"rock-paper-scissors-model.h5\")"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-a55953321c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueezenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSqueezeNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.squeezenet'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","metadata":{"id":"X3HHQVG3xzL0"},"source":["import cv2, sys\n","filepath = 'papel3.jpg'\n","\n","REV_CLASS_MAP = {\n","    0: \"piedra\",\n","    1: \"papel\",\n","    2: \"tijeras\",\n","    3: \"nada\"\n","}\n","\n","\n","def mapper(val):\n","    return REV_CLASS_MAP[val]\n","\n","# prepare the image\n","img = cv2.imread(filepath)\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","# predict the move made\n","pred = model.predict(np.array([img]))\n","move_code = np.argmax(pred[0])\n","move_name = mapper(move_code)\n","\n","print(\"Prediccion: {}\".format(move_name))"],"execution_count":null,"outputs":[]}]}